configuration:
  preprocessing:
    values:
    - None
    - standardization
    - normalization
    - minmax
  augmentation:
    values:
    - None
    - random
    - mixup
    - cutmix
  precision:
    values:
    - float16
    - float32 
    - float64
    - global_policy_float16
  batch_size:
    values:
    - 1
    - 16
    - 32
    - 64
  partitioning:
    values:
    - 60-20-20
    - 70-15-15
    - 80-10-10
    - 90-5-5
  lr: # convnext2 appendix
    values:
    - 0.01 # baseline SGD
    - 1.5e-4
    - 8.0e-4
    - 6.25e-3
  lr_schedule:
    values:
    - constant
    - exponential
    - polynomial
    - cosine
  optimizer_momentum:
    values:
    - 0.0
    - 0.5
    - 0.9
    - 0.99
  optimizer:
    values:
    - RMSProp
    - SGD
    - Adam
    - AdamW
  weight_decay: # See ConvNext2 appendix
    values:
    - 0.01
    - 0.001
    - 0.0001
    - 0.00001
  quantization:
    values:
    - None
    - pre
    - post_weights
    - post_weights_and_activations
  internal_optimizations:
    values:
    - None
    - weight_pruning
    - weight_clustering
    - jit_compilation

# weight pruning might be more beneficial for models 
# that are deployed on resource-constrained devices, 
# while weight clustering might be more beneficial 
# for models that need to be interpretable. !